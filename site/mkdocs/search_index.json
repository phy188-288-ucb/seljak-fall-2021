{
    "docs": [
        {
            "location": "/",
            "text": "University of California, Berkeley, Department of Physics\n\n\nPHY188/288: Fall 2020\n\n\nBayesian Data Analysis and Machine Learning for Physical Sciences\n\n\nInstructor: Uro\u0161 Seljak, useljak@berkeley.edu \n\n\u00a0 Office hours: Thursday 11:00AM-12:00PM\n\n\nGSI: Biwei Dai, biwei@berkeley.edu \n\n\u00a0 Office hours: Friday 1:00PM-2:00PM\n\n\nLecture: \nTuesday, Thursday 9:30AM-11:00AM, Online \n \n\nDiscussion: \nWednesday 1:00PM-2:00PM, Online \n \n\n\u00a0 \u00a0 \u00a0 \u00a0 or \nWednesday 2:00PM-3:00PM, Online \n \n\nUnits: \n4.0\n\n\nClass Number: 26276\n\n\nCourse Syllabus\n\n\nLearning goals\n:\nThe goals of the course is to get acquainted with modern computational methods\nused in physical sciences, including numerical analysis methods, data science and Bayesian statistics. \nWe will introduce a number of concepts that are useful in physical sciences at varying depth levels. We will cover main numerical methods used in physical sciences. Most of the statistical concepts will be Bayesian, \nemphasizing the concepts that have a connection to physical sciences, such as classical and statistical mechanics. We will focus on data science and data analysis applications that are often encountered in real world of physical sciences. We will review many of the machine learning concepts and methods. \n\n\nTarget audience\n:\nTarget student population are upper division undergraduates from physical science departments, as well as beginning graduate students from the same departments. The course is also suitable for graduate students looking for an introduction to programming and numerical \nmethods in phython.\n\n\nCourse structure\n:\nEach week there will be a set of 3 hour lectures discussing theoretical and practical underpinnings of the specific topic,\ntogether with its most common applications in physical sciences. Class participation is expected in the form of weekly reading the lecture in advance, submitting comments and questions on the lecture and answering a short set of questions. There will be eight python based homework assignments, applying the methods to physical science based applications. A weekly one hour discussion will focus on the lecture material and homeworks. There will be 3 longer projects spread over the term.\n\n\nPrerequsites\n: \n\nUndergraduate students: basic introduction to Python programming at the level of PHY77 or permission from instructor. \n\nGraduate students: none. \n\n\nGrades\n: 40% projects, 40% homeworks, 20% quizzes. \n\n\nWeekly Syllabus\n\n\nLecture 1: \nIntroduction to probability and Bayesian inference\n: general rules of probability, generating functions, moments and cumulants, binomial and multinomial, Poisson, gaussian distributions, multi-variate distributions, joint probability, marginal probability, Bayes theorem, forward and inverse probability, from probability to inference and the meaning of probability, prior, likelihood and posterior, interval estimates, comparison between Bayesian and classical statistics, Bayesian versus classical hypothesis testing (p-value) \n\n\n\u00a0 References: Ch. 2.1-2.3, 3 of MacKay, Ch. 2 of Kardar, Ch. 1-2 of Gelman et al, Bayesian data analysis\n\n\nLecture 2: \nMore on Bayesian inference and intro to data modeling\n: informative and noninformative priors, maximum a posteriori (MAP) and maximum likelihood estimator (MLE), asymptotic theorems, least square as MAP/MLE, fitting data to a straight line and a general linear least square model, normal equations\n\n\n\u00a0 Reference: Ch. 15 of NR, Ch. 3, 4 of Gelman et al. \n\n\nLecture 3: \nLinear Algebra\n: gaussian and Gauss-Jacobi elimination, backsubstitution, pivoting, LU decomposition, Cholesky decomposition, QR decomposition,  sparse matrix linear algebra, solving linear equations with linear algebra, QR decomposition and tridiagonal forms, diagonalization of a symmetric and non-symmetric matrix, principal axes and covariance matrix, singular value decomposition (SVD), application to normal equations, principal component analysis (PCA) and dimensionality reduction, independent component analysis (ICA)\n\n\n\u00a0 Reference: Ch. 2,11 of NR & Ch. 6 of Newman, https://arxiv.org/pdf/1404.2986.pdf\n\n\nLecture 4: \nInformation theory\n: Shannon information and mixing entropy, entropy for continuous variables and maximum entropy distributions, Kullback-Leibler divergence, negentropy, statistical independence, mutual and multi-information (application: FastICA), ensemble averaging: log likelihood as entropy,  curvature matrix (Hessian) as Fisher information matrix. Experiment design.  \n\n\n\u00a0 Reference: Ch. 2 of MacKay, Ch. 2 of Kardar, https://arxiv.org/pdf/1404.2986.pdf\n\n\nLecture 5: \nNonlinear equations and 1-d optimization\n: bisection, Newton-Raphson, secant, false position method. Golden ratio, parabolic optimization. Relaxation methods. \nOptimization in many dimensions\n: 1st order:gradient descent, stochastic gradient descent, mini-batch gradient descent. Momentum and Nesterov acceleration, ADAM. 2nd order methods: general strategies: choosing direction, doing line search or trust region. Newton, quasi-Newton, Gauss-Newton, conjugate gradient, Levenberg-Malmquardt method. \n\n\n\u00a0 Reference: Newman Ch. 6, NR Ch. 9, Nocedal & Wright, Optimization. NR 9,10,15. \n\n\nLecture 6: \nMonte Carlo methods for integration and posteriors\n: Simple Monte Carlo. Random number generators: transform method, Box-Muller for gaussian, Cholesky for multivariate gaussians, rejection sampling. Importance sampling for posteriors and for integration. Markov Chain Monte Carlo: Metropolis and Metropolis-Hastings. Convergence tests: burn-in, Gelman-Rubin statistic and chain correlation length. Improving efficiency: proposal function, Gibbs sampler with conditional conjugate distributions. Simulated annealing and simulated tampering. Hamiltonian Monte Carlo. Other MCMC approaches. \n\n\n\u00a0 References: NR, Press etal., Ch.7, Newman, Ch. 10, Gelman et al. Ch 10-12, MacKay Ch. 20-22\n\n\nLecture 7: \nMore advanced Bayesian analysis\n: probabilistic graphical models, hierarchical Bayesian models, model checking and evaluation, dealing with outliers\n\n\n\u00a0 References: Gelman et al. Ch. 5, 6, 7, 17\n\n\nLecture 8: \nVariational approximations\n: conditional and marginal approximations, expectation maximization and gaussian mixture model, variational inference and variational Bayes, expectation propagation\n\n\n\u00a0 References: Gelman Ch 13, 22\n\n\nLecture 9: \nBest practices of statistical analysis\n: Model checking, evaluating, Model comparison, bootstrap, jackknife, cross-validation tests, p-hacking, blind analysis, decision theory\n\n\n\u00a0 References: Gelman Ch 6-9, Mackay Ch 36\n\n\nLecture 10: \nInterpolation and extrapolation of data\n: polynomial, rational and spline interpolation, gaussian processes for regression and for classification\n\n\n\u00a0 References: NR Ch. 5, Gelman Ch. 21\n\n\nLecture 11: \nFourier methods\n: Fast Fourier transforms (FFT), FFT convolutions, power spectrum and correlation function, Wiener filtering and missing data, matched filtering, wavelets\n\n\n\u00a0 References: NR Ch. 12, 13\n\n\nLecture 12: \nClassification\n: supervised and unsupervised learning, naive Bayes, Decision Tree-Based methods, random forest\n\n\n\u00a0 References: Bishop Ch 3, Gelman Ch 20-21\n\n\nLecture 13: \nNeural networks, deep networks, Convolutional nets\n: neural networks, deep networks, adversarial networks and VAE, automated differentiation: back and forward propagation, inference: logistic function, ReLU\n\n\n\u00a0 References: http://cs231n.github.io/, https://arxiv.org/pdf/1803.08823.pdf, http://www.deeplearningbook.org/\n\n\nLiterature\n\n\n\n\n\n\nNumerical Recipes\n, by Press. W. et al.\n\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n, by David MacKay\n\n\n\n\n\n\nBayesian data analysis, 3rd edition, by Gelman A., et al.\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n, by James G. etal, \n\n\n\n\n\n\nA Survey of Computational Physics\n by Landau, R., Paez, M-J., Bordeianu, C.\n\n\n\n\n\n\nhttp://greenteapress.com/wp/think-bayes/\n\n\n\n\n\n\nhttps://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\n\n\n\n\n\n\nComputational Physics\n by Mark Newman\n\n\n\n\n\n\nEffective Computation in Physics, by A. Scopatz and K. D. Huff\n\n\n\n\n\n\nFrom Python to Numpy\n by N. P. Rougier\n\n\n\n\n\n\nOther resources will be provided according to the needs.\n\n\nSoftware\n\n\nWe will use Python Jupyter hub environment. You are expected to use existing numerical analysis routines and not write your own. Most of these are already\nimplemented in python libraries (scipy, numpy...).\n\n\nHomeworks and Projects\n\n\nThroughout the course, students will complete and electronically submit one homework assignment (code) per week.\nThis code will be run on test cases to check if it produces appropriate results. Students are encouraged to discuss \nhomeworks with other students, but should submit their own solutions. Homeworks will use the concepts developed in the \nlectures and apply to relatively simple problems. \n\n\nProjects are larger assignments intended to teach you how to combine ideas from the course in interesting ways and apply them \nto real data. There are three projects during the semester. You are encouraged to complete projects in pairs; your partner should be another student in your section. Work together to ensure that both group members understand the complete program you create.\n\n\nHomeworks and projects are to be submitted using Jupyter notebook in Python.\n\n\nSample projects: \n\n\n1) \nPlanck satellite data analysis\n: use measurement of Planck satellite power spectrum to determine cosmological parameters. First use linear algebra to solve the least square problem and find MAP/MLE best fit parameters. Next use optimization to solve for the same. Determine covariances of all parameters using Laplace approximation. Make predictions for future experiments with lower noise using Fisher matrix experiment design predictions. Use Planck published MCMC chains and analyze their burn-in phase, Gelman-Rubin statistics, and chain correlations. Plot their 1-d and 2-d distributions and compare them to MAP/Laplace approximation. Change one parameter and use importance sampling to produce new posteriors.  \n\n\n2) \nLIGO Nobel prize data analysis\n: use matched filtering methods and FFT to analyze first LIGO event and show it has detected gravitational waves.\n\n\n3) \nMachine learning on galaxies\n: use SDSS galaxy flux photometric measurements and redshift measurements to train the ML algorithms for regression, determining the redshift. Use verification data to test the training algorithms. Try KNN, gaussian processes, linear and quadratic regression, support vector machines, neural networks, random forest... Next try classification: use galaxy zoo galaxy morphology (spirals ellipticals, irregulars...) training data and apply to SDSS. Use photometry first, then add image information and observe how the accuracy improves.",
            "title": "Home"
        },
        {
            "location": "/#bayesian-data-analysis-and-machine-learning-for-physical-sciences",
            "text": "Instructor: Uro\u0161 Seljak, useljak@berkeley.edu  \n\u00a0 Office hours: Thursday 11:00AM-12:00PM  GSI: Biwei Dai, biwei@berkeley.edu  \n\u00a0 Office hours: Friday 1:00PM-2:00PM  Lecture:  Tuesday, Thursday 9:30AM-11:00AM, Online    \nDiscussion:  Wednesday 1:00PM-2:00PM, Online    \n\u00a0 \u00a0 \u00a0 \u00a0 or  Wednesday 2:00PM-3:00PM, Online    \nUnits:  4.0  Class Number: 26276",
            "title": "Bayesian Data Analysis and Machine Learning for Physical Sciences"
        },
        {
            "location": "/#course-syllabus",
            "text": "Learning goals :\nThe goals of the course is to get acquainted with modern computational methods\nused in physical sciences, including numerical analysis methods, data science and Bayesian statistics. \nWe will introduce a number of concepts that are useful in physical sciences at varying depth levels. We will cover main numerical methods used in physical sciences. Most of the statistical concepts will be Bayesian, \nemphasizing the concepts that have a connection to physical sciences, such as classical and statistical mechanics. We will focus on data science and data analysis applications that are often encountered in real world of physical sciences. We will review many of the machine learning concepts and methods.   Target audience :\nTarget student population are upper division undergraduates from physical science departments, as well as beginning graduate students from the same departments. The course is also suitable for graduate students looking for an introduction to programming and numerical \nmethods in phython.  Course structure :\nEach week there will be a set of 3 hour lectures discussing theoretical and practical underpinnings of the specific topic,\ntogether with its most common applications in physical sciences. Class participation is expected in the form of weekly reading the lecture in advance, submitting comments and questions on the lecture and answering a short set of questions. There will be eight python based homework assignments, applying the methods to physical science based applications. A weekly one hour discussion will focus on the lecture material and homeworks. There will be 3 longer projects spread over the term.  Prerequsites :  \nUndergraduate students: basic introduction to Python programming at the level of PHY77 or permission from instructor.  \nGraduate students: none.   Grades : 40% projects, 40% homeworks, 20% quizzes.",
            "title": "Course Syllabus"
        },
        {
            "location": "/#weekly-syllabus",
            "text": "Lecture 1:  Introduction to probability and Bayesian inference : general rules of probability, generating functions, moments and cumulants, binomial and multinomial, Poisson, gaussian distributions, multi-variate distributions, joint probability, marginal probability, Bayes theorem, forward and inverse probability, from probability to inference and the meaning of probability, prior, likelihood and posterior, interval estimates, comparison between Bayesian and classical statistics, Bayesian versus classical hypothesis testing (p-value)   \u00a0 References: Ch. 2.1-2.3, 3 of MacKay, Ch. 2 of Kardar, Ch. 1-2 of Gelman et al, Bayesian data analysis  Lecture 2:  More on Bayesian inference and intro to data modeling : informative and noninformative priors, maximum a posteriori (MAP) and maximum likelihood estimator (MLE), asymptotic theorems, least square as MAP/MLE, fitting data to a straight line and a general linear least square model, normal equations  \u00a0 Reference: Ch. 15 of NR, Ch. 3, 4 of Gelman et al.   Lecture 3:  Linear Algebra : gaussian and Gauss-Jacobi elimination, backsubstitution, pivoting, LU decomposition, Cholesky decomposition, QR decomposition,  sparse matrix linear algebra, solving linear equations with linear algebra, QR decomposition and tridiagonal forms, diagonalization of a symmetric and non-symmetric matrix, principal axes and covariance matrix, singular value decomposition (SVD), application to normal equations, principal component analysis (PCA) and dimensionality reduction, independent component analysis (ICA)  \u00a0 Reference: Ch. 2,11 of NR & Ch. 6 of Newman, https://arxiv.org/pdf/1404.2986.pdf  Lecture 4:  Information theory : Shannon information and mixing entropy, entropy for continuous variables and maximum entropy distributions, Kullback-Leibler divergence, negentropy, statistical independence, mutual and multi-information (application: FastICA), ensemble averaging: log likelihood as entropy,  curvature matrix (Hessian) as Fisher information matrix. Experiment design.    \u00a0 Reference: Ch. 2 of MacKay, Ch. 2 of Kardar, https://arxiv.org/pdf/1404.2986.pdf  Lecture 5:  Nonlinear equations and 1-d optimization : bisection, Newton-Raphson, secant, false position method. Golden ratio, parabolic optimization. Relaxation methods.  Optimization in many dimensions : 1st order:gradient descent, stochastic gradient descent, mini-batch gradient descent. Momentum and Nesterov acceleration, ADAM. 2nd order methods: general strategies: choosing direction, doing line search or trust region. Newton, quasi-Newton, Gauss-Newton, conjugate gradient, Levenberg-Malmquardt method.   \u00a0 Reference: Newman Ch. 6, NR Ch. 9, Nocedal & Wright, Optimization. NR 9,10,15.   Lecture 6:  Monte Carlo methods for integration and posteriors : Simple Monte Carlo. Random number generators: transform method, Box-Muller for gaussian, Cholesky for multivariate gaussians, rejection sampling. Importance sampling for posteriors and for integration. Markov Chain Monte Carlo: Metropolis and Metropolis-Hastings. Convergence tests: burn-in, Gelman-Rubin statistic and chain correlation length. Improving efficiency: proposal function, Gibbs sampler with conditional conjugate distributions. Simulated annealing and simulated tampering. Hamiltonian Monte Carlo. Other MCMC approaches.   \u00a0 References: NR, Press etal., Ch.7, Newman, Ch. 10, Gelman et al. Ch 10-12, MacKay Ch. 20-22  Lecture 7:  More advanced Bayesian analysis : probabilistic graphical models, hierarchical Bayesian models, model checking and evaluation, dealing with outliers  \u00a0 References: Gelman et al. Ch. 5, 6, 7, 17  Lecture 8:  Variational approximations : conditional and marginal approximations, expectation maximization and gaussian mixture model, variational inference and variational Bayes, expectation propagation  \u00a0 References: Gelman Ch 13, 22  Lecture 9:  Best practices of statistical analysis : Model checking, evaluating, Model comparison, bootstrap, jackknife, cross-validation tests, p-hacking, blind analysis, decision theory  \u00a0 References: Gelman Ch 6-9, Mackay Ch 36  Lecture 10:  Interpolation and extrapolation of data : polynomial, rational and spline interpolation, gaussian processes for regression and for classification  \u00a0 References: NR Ch. 5, Gelman Ch. 21  Lecture 11:  Fourier methods : Fast Fourier transforms (FFT), FFT convolutions, power spectrum and correlation function, Wiener filtering and missing data, matched filtering, wavelets  \u00a0 References: NR Ch. 12, 13  Lecture 12:  Classification : supervised and unsupervised learning, naive Bayes, Decision Tree-Based methods, random forest  \u00a0 References: Bishop Ch 3, Gelman Ch 20-21  Lecture 13:  Neural networks, deep networks, Convolutional nets : neural networks, deep networks, adversarial networks and VAE, automated differentiation: back and forward propagation, inference: logistic function, ReLU  \u00a0 References: http://cs231n.github.io/, https://arxiv.org/pdf/1803.08823.pdf, http://www.deeplearningbook.org/",
            "title": "Weekly Syllabus"
        },
        {
            "location": "/#literature",
            "text": "Numerical Recipes , by Press. W. et al.    Information Theory, Inference and Learning Algorithms , by David MacKay    Bayesian data analysis, 3rd edition, by Gelman A., et al.    An Introduction to Statistical Learning , by James G. etal,     A Survey of Computational Physics  by Landau, R., Paez, M-J., Bordeianu, C.    http://greenteapress.com/wp/think-bayes/    https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers    Computational Physics  by Mark Newman    Effective Computation in Physics, by A. Scopatz and K. D. Huff    From Python to Numpy  by N. P. Rougier    Other resources will be provided according to the needs.",
            "title": "Literature"
        },
        {
            "location": "/#software",
            "text": "We will use Python Jupyter hub environment. You are expected to use existing numerical analysis routines and not write your own. Most of these are already\nimplemented in python libraries (scipy, numpy...).",
            "title": "Software"
        },
        {
            "location": "/#homeworks-and-projects",
            "text": "Throughout the course, students will complete and electronically submit one homework assignment (code) per week.\nThis code will be run on test cases to check if it produces appropriate results. Students are encouraged to discuss \nhomeworks with other students, but should submit their own solutions. Homeworks will use the concepts developed in the \nlectures and apply to relatively simple problems.   Projects are larger assignments intended to teach you how to combine ideas from the course in interesting ways and apply them \nto real data. There are three projects during the semester. You are encouraged to complete projects in pairs; your partner should be another student in your section. Work together to ensure that both group members understand the complete program you create.  Homeworks and projects are to be submitted using Jupyter notebook in Python.  Sample projects:   1)  Planck satellite data analysis : use measurement of Planck satellite power spectrum to determine cosmological parameters. First use linear algebra to solve the least square problem and find MAP/MLE best fit parameters. Next use optimization to solve for the same. Determine covariances of all parameters using Laplace approximation. Make predictions for future experiments with lower noise using Fisher matrix experiment design predictions. Use Planck published MCMC chains and analyze their burn-in phase, Gelman-Rubin statistics, and chain correlations. Plot their 1-d and 2-d distributions and compare them to MAP/Laplace approximation. Change one parameter and use importance sampling to produce new posteriors.    2)  LIGO Nobel prize data analysis : use matched filtering methods and FFT to analyze first LIGO event and show it has detected gravitational waves.  3)  Machine learning on galaxies : use SDSS galaxy flux photometric measurements and redshift measurements to train the ML algorithms for regression, determining the redshift. Use verification data to test the training algorithms. Try KNN, gaussian processes, linear and quadratic regression, support vector machines, neural networks, random forest... Next try classification: use galaxy zoo galaxy morphology (spirals ellipticals, irregulars...) training data and apply to SDSS. Use photometry first, then add image information and observe how the accuracy improves.",
            "title": "Homeworks and Projects"
        },
        {
            "location": "/homeworks/",
            "text": "Homeworks\n\n\nThis page contains a list of links to PHY188/288 homeworks.\n\n\nYou can also access assignments from a link posted on the  bCourses website,  under \u201cAssignments\",\nwhich contains the most updated information. \n\n\n\n\n\n\nPython Tutorial\n: \n\n \u00a0 \u00a0 \nIntro to Python\n (source:  https://github.com/berkeley-physics/intro_python)\n\n\n\n\n\n\nHW1 (Intro to Statistics)\n (Due Sept. 09, 11:59pm): \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n/    \nSolution\n\n\n\n\n\n\nHW2 (Intro to Data Analysis, Dimensionality Reduction, and Clustering):\n (Due Sept. 16, 11:59pm): \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n/    \nSolution\n\n\n\n\n\n\nHW3 (Linear Algebra - Gaussian Elimination, SVD, Polynomial Regression, PCA, KNN, and Data Modeling)\n (Due Sept. 23, 11:59pm): \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n/    \nSolution\n\n\n\n\n\n\nHW4 (Fisher Information Matrix & Independent Component Analysis)\n (Due Sept. 30, 11:59pm): \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n/    \nSolution\n\n\n\n\n\n\nProject1-Part1 (Planck analysis - Linear Algebra & Optimization)\n (Due Oct. 9, 11:59pm): \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n/    \nSolution\n\n\n\n\n\n\n10/9/2020: Problem 2 in this project is now slightly modified. If you already started working on it, You should take your codes from the old link and re-run them in the new link. \n\n\n\n\nProject1-Part2 (Planck analysis II + Supernovae Project)\n (Due Oct. 16, 11:59pm): \n \n \u00a0 \u00a0 \nNEW LINK - 188\n/    \nNEW LINK - 288\n/    \nSolution\n\n \n \u00a0 \u00a0 \nOLD LINK - 188\n/    \nOLD LINK - 288\n\n\n\n\nLinks to HW5 and all future assignments will be available only on bcourses (under \"Assignments\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\n\n\nFor the remainder of the semester, you are going to be writing and running the homeworks remotely on Google Colaboratory.\nThe greatest advantage of using Colab is that it supports free GPU, and this will be particularly useful for future assignments.\n\n\nFirst, make sure that you are logged in to your Google Drive account. \nOpen https://colab.research.google.com, and you can go over the Colab introduction notebook. \n\n\nGo to your personal google drive and create a folder for this class. Important: \nName it as \"P188_288\"\n\n\n\n\nOn bcourses (under \"Assignments\"), you will be given a link to a zip file containing Jupyter notebooks. Upzip it and upload the entire assignment folder to your Drive folder \"P188_288\". \n\n\n\nNext, find the assignment notebook (files ending in .ipynb) and double click on it. Open with Colab. \n\n\n\nCAUTION: Make sure to save your work progress. Note that the Colab will disconnect if you are idle for a certain amount of time or if your total connection time exceeds the max allowed time. If that happens, any unsaved progress will be lost. Hence, please get in the habit of saving your code frequently (File -> Save).\n \n\n\nOnce you finish your homework, you are going to submit the following:\n\n\n\n\n\n\nJupyter notebook converted to a pdf file\n\n\n\n\n\n\nPublicly accessible link to your notebook\n\n\n\n\n\n\nFirst, download your notebook as a pdf file. (File > Print > Save as PDF) \nImportant: \nBefore you download your notebook, make sure to display full outputs in Jupyter. Check if your pdf file displays both codes and outputs.\n\n\n\n\nTo get a publicly accessible link, hit the Share button at the top right, then click \"Get link.\" Make sure that your link is publicly available. (Click on \"Anyone with the link\") Copy link. \n\n\n\nGo to bcourses and click the corresponding assignment title. Next, click the \nSubmit Assignment\n button.\n\n\n\nClick the \nChoose File\n button to upload a notebook (pdf file) from your computer. \n\n\n\nProvide a link to your Colab notebook. \n\n\n\nclick the \nSubmit Assignment\n button, and make sure to view the confirmation of your submission.\n\n\n\n\n\nPast Assignments\n\n\n\n\nHW1 (Numerical Integration and ODE/PDEs)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW2 (Intro to Statistics)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW3 (Intro to Data Analysis, Dimensionality Reduction, and Clustering)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW4 (Linear Algebra - Gaussian Elimination, SVD, Polynomial Regression, PCA, KNN, and Data Modeling)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW5 (Fisher Information Matrix & Independent Component Analysis)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nProject1-Part1 (Planck analysis I - Linear Algebra & Optimization)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nProject1-Part2 (Planck analysis II - Bayesfast & Markov Chain Monte Carlo)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW6 (Markov Chain Simulation and Hierarchical Model)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW7 (Distributional Approximation, Expectation Maximization (EM), Interpolation and Resampling Methods)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW8 part 1 (VI, EL2O, Generative Models, Multimodal Posteriors, and Gaussian Processes)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nProject 2 (LIGO analysis - Fourier methods, Matched Filtering, and Differential Equations)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nHW8 part 2 (Linear Regression, Regularization, and Logistic & Softmax Regression)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288\n\n\nProject 3 (Final) (Classification and inference with machine learning)\n: \n \n \u00a0 \u00a0 \nPhysics 188\n/    \nPhysics 288",
            "title": "Homeworks"
        },
        {
            "location": "/homeworks/#homeworks",
            "text": "This page contains a list of links to PHY188/288 homeworks.  You can also access assignments from a link posted on the  bCourses website,  under \u201cAssignments\",\nwhich contains the most updated information.     Python Tutorial :   \u00a0 \u00a0  Intro to Python  (source:  https://github.com/berkeley-physics/intro_python)    HW1 (Intro to Statistics)  (Due Sept. 09, 11:59pm): \n   \u00a0 \u00a0  Physics 188 /     Physics 288 /     Solution    HW2 (Intro to Data Analysis, Dimensionality Reduction, and Clustering):  (Due Sept. 16, 11:59pm): \n   \u00a0 \u00a0  Physics 188 /     Physics 288 /     Solution    HW3 (Linear Algebra - Gaussian Elimination, SVD, Polynomial Regression, PCA, KNN, and Data Modeling)  (Due Sept. 23, 11:59pm): \n   \u00a0 \u00a0  Physics 188 /     Physics 288 /     Solution    HW4 (Fisher Information Matrix & Independent Component Analysis)  (Due Sept. 30, 11:59pm): \n   \u00a0 \u00a0  Physics 188 /     Physics 288 /     Solution    Project1-Part1 (Planck analysis - Linear Algebra & Optimization)  (Due Oct. 9, 11:59pm): \n   \u00a0 \u00a0  Physics 188 /     Physics 288 /     Solution    10/9/2020: Problem 2 in this project is now slightly modified. If you already started working on it, You should take your codes from the old link and re-run them in the new link.    Project1-Part2 (Planck analysis II + Supernovae Project)  (Due Oct. 16, 11:59pm): \n   \u00a0 \u00a0  NEW LINK - 188 /     NEW LINK - 288 /     Solution \n   \u00a0 \u00a0  OLD LINK - 188 /     OLD LINK - 288   Links to HW5 and all future assignments will be available only on bcourses (under \"Assignments\")",
            "title": "Homeworks"
        },
        {
            "location": "/homeworks/#instructions",
            "text": "For the remainder of the semester, you are going to be writing and running the homeworks remotely on Google Colaboratory.\nThe greatest advantage of using Colab is that it supports free GPU, and this will be particularly useful for future assignments.  First, make sure that you are logged in to your Google Drive account. \nOpen https://colab.research.google.com, and you can go over the Colab introduction notebook.   Go to your personal google drive and create a folder for this class. Important:  Name it as \"P188_288\"   On bcourses (under \"Assignments\"), you will be given a link to a zip file containing Jupyter notebooks. Upzip it and upload the entire assignment folder to your Drive folder \"P188_288\".   Next, find the assignment notebook (files ending in .ipynb) and double click on it. Open with Colab.   CAUTION: Make sure to save your work progress. Note that the Colab will disconnect if you are idle for a certain amount of time or if your total connection time exceeds the max allowed time. If that happens, any unsaved progress will be lost. Hence, please get in the habit of saving your code frequently (File -> Save).    Once you finish your homework, you are going to submit the following:    Jupyter notebook converted to a pdf file    Publicly accessible link to your notebook    First, download your notebook as a pdf file. (File > Print > Save as PDF) \nImportant:  Before you download your notebook, make sure to display full outputs in Jupyter. Check if your pdf file displays both codes and outputs.   To get a publicly accessible link, hit the Share button at the top right, then click \"Get link.\" Make sure that your link is publicly available. (Click on \"Anyone with the link\") Copy link.   Go to bcourses and click the corresponding assignment title. Next, click the  Submit Assignment  button.  Click the  Choose File  button to upload a notebook (pdf file) from your computer.   Provide a link to your Colab notebook.   click the  Submit Assignment  button, and make sure to view the confirmation of your submission.",
            "title": "Instructions"
        },
        {
            "location": "/homeworks/#past-assignments",
            "text": "HW1 (Numerical Integration and ODE/PDEs) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW2 (Intro to Statistics) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW3 (Intro to Data Analysis, Dimensionality Reduction, and Clustering) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW4 (Linear Algebra - Gaussian Elimination, SVD, Polynomial Regression, PCA, KNN, and Data Modeling) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW5 (Fisher Information Matrix & Independent Component Analysis) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  Project1-Part1 (Planck analysis I - Linear Algebra & Optimization) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  Project1-Part2 (Planck analysis II - Bayesfast & Markov Chain Monte Carlo) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW6 (Markov Chain Simulation and Hierarchical Model) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW7 (Distributional Approximation, Expectation Maximization (EM), Interpolation and Resampling Methods) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW8 part 1 (VI, EL2O, Generative Models, Multimodal Posteriors, and Gaussian Processes) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  Project 2 (LIGO analysis - Fourier methods, Matched Filtering, and Differential Equations) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  HW8 part 2 (Linear Regression, Regularization, and Logistic & Softmax Regression) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288  Project 3 (Final) (Classification and inference with machine learning) : \n   \u00a0 \u00a0  Physics 188 /     Physics 288   <!-- Past Assignments:\nTo download a Jupyter notebook, right click the link and save it as an .ipynb file. <!-- - **HW1** (Numerical Integration and ODE/PDEs): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW1.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW1.ipynb) <!-- - **HW2** (Intro to Statistics): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW2.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW2.ipynb) <!-- - **HW3** (Intro to Statistics - Part 2): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW3.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW3.ipynb) <!-- - **HW4** (Linear Algebra and Data Modeling): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW4.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW4.ipynb) <!-- - **HW5** (Markov Chain Simulation and Hierarchical Model): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW5.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW5.ipynb) <!-- - **Project 1 - part 1** (Fisher Information Matrix): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project1_p1.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project1_p1.ipynb) <!-- - **Project 1 - part 2** (Linear Algebra and Optimization): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project1_p2.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project1_p2.ipynb) <!-- - **Project 1 - part 1** (Markov chain Monte Carlo): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project1_p3.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project1_p3.ipynb) <!-- - **HW6** (MLE, MCMC, Interpolation, Expectation Maximization (EM), and Resampling Methods): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW6.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW6.ipynb) <!-- - **HW7** (Distributional Approximation and Gaussian Processes): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW7.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW7.ipynb) <!-- - **Project 2** (Fourier methods, Matched Filtering, and Differential Equations): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project2.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project2.ipynb) <!-- - **HW8** (Linear Regression, Regularization, and Logistic & Softmax Regression): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW8.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/HW8.ipynb) < !-- - **Project 3** (Classification and inference with machine learning): [PDF](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project3.pdf)/  [Jupyter notebook](https://raw.githubusercontent.com/phy188-288-ucb/seljak-fall-2019/master/Homework/Project3.ipynb)",
            "title": "Past Assignments"
        },
        {
            "location": "/lectures/",
            "text": "Lecture Notes\n\n\nThis page\n contains a list of links to PHY188/288 lecture notes.\n\n\n\n\nLecture 1: Intro to Statistics\n\n\nLecture 2: Intro to Data Analysis and Machine Learning\n\n\nLecture 3: Linear Algebra\n\n\nLecture 4: Information Theory, Entropy, Experiment Design\n\n\nLecture 5: Nonlinear Equations and Optimization\n \n\n\nLecture 6: Monte Carlo Sampling and Integration\n   \n\n\nLecture 7: Advanced Bayesian Concepts (Probabilistic graphical models, Hierarchical Bayesian models, etc)\n\n\nLecture 8: Distributional Approximation\n\n\nLecture 9: Useful Statistical Methods of Data Analysis\n\n\nLecture 10: From Interpolation to Regressions to Gaussian Processes\n\n\nLecture 11: Classification\n\n\nLecture 12: Fourier Methods\n \n\n\nLecture 13: Neural Networks for Supervised Learning\n \n\n\nLecture 14: Deep Networks for Unsupervised Learning",
            "title": "Lectures"
        },
        {
            "location": "/lectures/#lecture-notes",
            "text": "This page  contains a list of links to PHY188/288 lecture notes.   Lecture 1: Intro to Statistics  Lecture 2: Intro to Data Analysis and Machine Learning  Lecture 3: Linear Algebra  Lecture 4: Information Theory, Entropy, Experiment Design  Lecture 5: Nonlinear Equations and Optimization    Lecture 6: Monte Carlo Sampling and Integration      Lecture 7: Advanced Bayesian Concepts (Probabilistic graphical models, Hierarchical Bayesian models, etc)  Lecture 8: Distributional Approximation  Lecture 9: Useful Statistical Methods of Data Analysis  Lecture 10: From Interpolation to Regressions to Gaussian Processes  Lecture 11: Classification  Lecture 12: Fourier Methods    Lecture 13: Neural Networks for Supervised Learning    Lecture 14: Deep Networks for Unsupervised Learning      < !-- A full list can be found at on [github](https://github.com/phy188-288-ucb/seljak-fall-2019/tree/master/lecture-notes/)",
            "title": "Lecture Notes"
        }
    ]
}